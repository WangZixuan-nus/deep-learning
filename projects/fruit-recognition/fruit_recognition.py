# -*- coding: utf-8 -*-
"""fruit_recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_BoRC3G45eYkv4xF27RacT_gp-Tp1u97
"""

import os
import time
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

!git clone https://github.com/WangZixuan-nus/deep-learning.git


# Define the data directories
TRAIN_DIR = "/content/deep-learning/projects/fruit-recognition/fruitdata/train"
TEST_DIR = "/content/deep-learning/projects/fruit-recognition/fruitdata/test"
AUGMENTED_TRAIN_DIR = "/content/augmented_data/train"
AUGMENTED_TEST_DIR = "/content/augmented_data/test"

def setup_directories():
    """
    Creates necessary directories for saving results and models
    """
    # Create directories if they don't exist
    os.makedirs("/content/augmented_data/train", exist_ok=True)
    os.makedirs("/content/augmented_data/test", exist_ok=True)
    os.makedirs("/content/models", exist_ok=True)
    os.makedirs("/content/results", exist_ok=True)

def augment_data(source_dir, target_dir, augmentation_factor=3):
    """
    Augments images in source_dir and saves them to target_dir

    Parameters:
    - source_dir: Directory containing original images
    - target_dir: Directory where augmented images will be saved
    - augmentation_factor: Number of augmented images to generate per original image
    """
    print(f"Augmenting data from {source_dir} to {target_dir}...")

    # Define the image data generator with various transformations
    datagen = ImageDataGenerator(
        rotation_range=40,        # Random rotation in the range of 40 degrees
        width_shift_range=0.2,    # Random horizontal shift up to 20%
        height_shift_range=0.2,   # Random vertical shift up to 20%
        shear_range=0.2,          # Random shearing transformations
        zoom_range=0.2,           # Random zoom up to 20%
        horizontal_flip=True,     # Randomly flip images horizontally
        fill_mode='nearest'       # Fill in newly created pixels using nearest neighbor approach
    )

    # Process each class directory
    for class_name in os.listdir(source_dir):
        # Skip non-directory files
        if not os.path.isdir(os.path.join(source_dir, class_name)):
            continue

        # Create class directory in target_dir if it doesn't exist
        class_target_dir = os.path.join(target_dir, class_name)
        os.makedirs(class_target_dir, exist_ok=True)

        # Copy original images first
        class_source_dir = os.path.join(source_dir, class_name)
        for img_name in os.listdir(class_source_dir):
            if img_name.endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(class_source_dir, img_name)
                # Load the image
                img = tf.keras.preprocessing.image.load_img(img_path)
                # Convert to array
                x = tf.keras.preprocessing.image.img_to_array(img)
                # Reshape to (1, height, width, channels)
                x = x.reshape((1,) + x.shape)

                # Copy the original image
                tf.keras.preprocessing.image.save_img(
                    os.path.join(class_target_dir, img_name),
                    x[0]
                )

                # Generate augmented images
                i = 0
                for batch in datagen.flow(
                    x,
                    batch_size=1,
                    save_to_dir=class_target_dir,
                    save_prefix=f"{os.path.splitext(img_name)[0]}_aug",
                    save_format='jpg'
                ):
                    i += 1
                    if i >= augmentation_factor:
                        break

    print(f"Data augmentation completed. Augmented images saved to {target_dir}")

def data_load(data_dir, test_data_dir, img_height, img_width, batch_size):
    """
    Loads the training and testing datasets

    Parameters:
    - data_dir: Directory containing training images
    - test_data_dir: Directory containing testing images
    - img_height: Target image height
    - img_width: Target image width
    - batch_size: Batch size for training/testing

    Returns:
    - train_ds: Training dataset
    - val_ds: Validation/testing dataset
    - class_names: List of class names
    """
    # Load training dataset
    train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        label_mode='categorical',
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size
    )

    # Load testing dataset
    val_ds = tf.keras.preprocessing.image_dataset_from_directory(
        test_data_dir,
        label_mode='categorical',
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size
    )

    # Get class names from the training dataset
    class_names = train_ds.class_names

    return train_ds, val_ds, class_names

def build_cnn_model(img_shape=(224, 224, 3), class_num=15):
    """
    Builds and compiles a CNN model

    Parameters:
    - img_shape: Shape of input images (height, width, channels)
    - class_num: Number of classes to predict

    Returns:
    - model: Compiled CNN model
    """
    model = tf.keras.models.Sequential([
        # Normalize pixel values to [0, 1]
        # Updated from experimental.preprocessing.Rescaling to just Rescaling
        tf.keras.layers.Rescaling(1./255, input_shape=img_shape),

        # First convolutional block
        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),  # 16 filters with 3x3 kernel
        tf.keras.layers.MaxPooling2D(2, 2),                     # Reduces spatial dimensions by 2

        # Second convolutional block
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),  # 32 filters with 3x3 kernel
        tf.keras.layers.MaxPooling2D(2, 2),                     # Reduces spatial dimensions by 2

        # Third convolutional block
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # 64 filters with 3x3 kernel
        tf.keras.layers.MaxPooling2D(2, 2),                     # Reduces spatial dimensions by 2

        # Flatten the output for the dense layers
        tf.keras.layers.Flatten(),

        # Fully connected layer with 128 units
        tf.keras.layers.Dense(128, activation='relu'),

        # Output layer with softmax activation for multi-class classification
        tf.keras.layers.Dense(class_num, activation='softmax')
    ])

    # Display the model architecture
    model.summary()

    # Compile the model with SGD optimizer and categorical crossentropy loss
    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def build_mobilenet_model(img_shape=(224, 224, 3), class_num=15):
    """
    Builds and compiles a MobileNetV2-based model using transfer learning

    Parameters:
    - img_shape: Shape of input images (height, width, channels)
    - class_num: Number of classes to predict

    Returns:
    - model: Compiled MobileNetV2 model
    """
    # Load pre-trained MobileNetV2 without the top classifier
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=img_shape,
        include_top=False,
        weights='imagenet'
    )

    # Freeze the base model so its weights won't be updated during training
    base_model.trainable = False

    # Build the complete model
    model = tf.keras.models.Sequential([
        # Preprocessing layer to scale pixel values to [-1, 1] as expected by MobileNetV2
        # Updated from experimental.preprocessing.Rescaling to just Rescaling
        tf.keras.layers.Rescaling(1./127.5, offset=-1, input_shape=img_shape),

        # Pre-trained base model
        base_model,

        # Global average pooling to reduce spatial dimensions
        tf.keras.layers.GlobalAveragePooling2D(),

        # Output layer with softmax activation for multi-class classification
        tf.keras.layers.Dense(class_num, activation='softmax')
    ])

    # Display the model architecture
    model.summary()

    # Compile the model with Adam optimizer and categorical crossentropy loss
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def show_accuracy_and_loss(history, model_name="model"):
    """
    Plots the training and validation accuracy and loss

    Parameters:
    - history: History object returned by model.fit()
    - model_name: Name of the model for saving the plot
    """
    # Extract metrics from history
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    # Create plot
    plt.figure(figsize=(10, 8))

    # Plot accuracy
    plt.subplot(2, 1, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.ylabel('Accuracy')
    plt.ylim([min(plt.ylim()), 1])
    plt.title('Training and Validation Accuracy')

    # Plot loss
    plt.subplot(2, 1, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.ylabel('Cross Entropy')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')

    # Save the plot
    plt.tight_layout()
    filename = f"/content/results/results_{model_name}.png"
    plt.savefig(filename, dpi=100)
    print(f"Training results plot saved to {filename}")
    plt.close()

def train_model(model_type="cnn", epochs=15):
    """
    Trains either a CNN or MobileNet model

    Parameters:
    - model_type: Type of model to train ("cnn" or "mobilenet")
    - epochs: Number of training epochs
    """
    start_time = time.time()

    # Load data from augmented directories
    train_ds, val_ds, class_names = data_load(
        AUGMENTED_TRAIN_DIR,
        AUGMENTED_TEST_DIR,
        224, 224, 16
    )

    print(f"Training {model_type.upper()} model with {len(class_names)} classes: {class_names}")

    # Build the model based on type
    if model_type == "cnn":
        model = build_cnn_model(class_num=len(class_names))
        model_path = "/content/models/cnn_model.h5"
    else:
        model = build_mobilenet_model(class_num=len(class_names))
        model_path = "/content/models/mobilenet_model.h5"

    # Train the model
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs
    )

    # Save the model
    model.save(model_path)
    print(f"Model saved to {model_path}")

    # Calculate training time
    end_time = time.time()
    duration = end_time - start_time
    print(f'Training completed in {duration:.2f} seconds')

    # Show and save training progress
    show_accuracy_and_loss(history, model_type)

    return model, class_names

def evaluate_model(model_path, model_name):
    """
    Evaluates a trained model and generates a confusion matrix heatmap

    Parameters:
    - model_path: Path to the saved model
    - model_name: Name of the model for saving results
    """
    # Load dataset and model
    _, test_ds, class_names = data_load(
        AUGMENTED_TRAIN_DIR,
        AUGMENTED_TEST_DIR,
        224, 224, 16
    )

    model = tf.keras.models.load_model(model_path)

    # Evaluate model on test data
    loss, accuracy = model.evaluate(test_ds)
    print(f'{model_name.upper()} Test Accuracy: {accuracy:.4f}')

    # Collect predictions and true labels
    true_labels = []
    predicted_labels = []

    for images, labels in test_ds:
        true_batch = labels.numpy()
        predictions = model.predict(images)

        true_indices = np.argmax(true_batch, axis=1)
        predicted_indices = np.argmax(predictions, axis=1)

        true_labels.extend(true_indices)
        predicted_labels.extend(predicted_indices)

    # Create confusion matrix
    num_classes = len(class_names)
    confusion_matrix = np.zeros((num_classes, num_classes))

    for true, pred in zip(true_labels, predicted_labels):
        confusion_matrix[true][pred] += 1

    # Normalize by row (true labels)
    row_sums = confusion_matrix.sum(axis=1, keepdims=True)
    normalized_matrix = confusion_matrix / row_sums

    # Plot heatmap
    plt.figure(figsize=(12, 10))
    ax = plt.subplot()
    im = ax.imshow(normalized_matrix, cmap="OrRd")

    # Add labels
    ax.set_xticks(np.arange(len(class_names)))
    ax.set_yticks(np.arange(len(class_names)))
    ax.set_xticklabels(class_names, rotation=45, ha="right", rotation_mode="anchor")
    ax.set_yticklabels(class_names)

    # Add text annotations
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            text = ax.text(j, i, f"{normalized_matrix[i, j]:.2f}",
                           ha="center", va="center", color="black")

    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")
    ax.set_title(f"Confusion Matrix - {model_name.upper()}")

    plt.tight_layout()
    plt.colorbar(im)

    # Save heatmap
    filename = f"/content/results/heatmap_{model_name}.png"
    plt.savefig(filename, dpi=100)
    print(f"Confusion matrix heatmap saved to {filename}")
    plt.close()

def compare_models():
    """
    Compares the performance of CNN and MobileNet models
    """
    # Load results
    cnn_model = tf.keras.models.load_model("/content/models/cnn_model.h5")
    mobilenet_model = tf.keras.models.load_model("/content/models/mobilenet_model.h5")

    _, test_ds, _ = data_load(
        AUGMENTED_TRAIN_DIR,
        AUGMENTED_TEST_DIR,
        224, 224, 16
    )

    # Evaluate models
    cnn_loss, cnn_accuracy = cnn_model.evaluate(test_ds)
    mobilenet_loss, mobilenet_accuracy = mobilenet_model.evaluate(test_ds)

    # Create comparison plot
    models = ['CNN', 'MobileNetV2']
    accuracies = [cnn_accuracy, mobilenet_accuracy]

    plt.figure(figsize=(10, 6))
    plt.bar(models, accuracies, color=['#3498db', '#2ecc71'])
    plt.ylim([0, 1])
    plt.xlabel('Model')
    plt.ylabel('Accuracy')
    plt.title('Model Accuracy Comparison')

    # Add text labels on bars
    for i, acc in enumerate(accuracies):
        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')

    # Save comparison
    filename = "/content/results/model_comparison.png"
    plt.savefig(filename, dpi=100)
    print(f"Model comparison saved to {filename}")
    plt.close()

    # Print comparison results
    print("\nModel Comparison Results:")
    print(f"CNN Accuracy: {cnn_accuracy:.4f}")
    print(f"MobileNetV2 Accuracy: {mobilenet_accuracy:.4f}")
    print(f"Difference: {abs(cnn_accuracy - mobilenet_accuracy):.4f}")

    if cnn_accuracy > mobilenet_accuracy:
        print("CNN model performs better on this dataset.")
    elif mobilenet_accuracy > cnn_accuracy:
        print("MobileNetV2 model performs better on this dataset.")
    else:
        print("Both models perform equally on this dataset.")

def main():
    """
    Main function to execute the complete workflow
    """
    # Setup directories
    setup_directories()

    # Augment training and testing data
    augment_data(TRAIN_DIR, AUGMENTED_TRAIN_DIR, augmentation_factor=3)
    augment_data(TEST_DIR, AUGMENTED_TEST_DIR, augmentation_factor=2)

    # Train CNN model
    print("\n===== Training CNN Model =====")
    _, _ = train_model(model_type="cnn", epochs=15)

    # Train MobileNet model
    print("\n===== Training MobileNet Model =====")
    _, _ = train_model(model_type="mobilenet", epochs=15)

    # Evaluate models
    print("\n===== Evaluating CNN Model =====")
    evaluate_model("/content/models/cnn_model.h5", "cnn")

    print("\n===== Evaluating MobileNet Model =====")
    evaluate_model("/content/models/mobilenet_model.h5", "mobilenet")

    # Compare models
    print("\n===== Comparing Models =====")
    compare_models()

if __name__ == "__main__":
    main()