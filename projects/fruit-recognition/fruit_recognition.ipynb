{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mfY-3t2udawO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras import Input, Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1zDx1j3dg1m",
        "outputId": "1abef906-4af0-4e60-c1a1-0450e574119a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning'...\n",
            "remote: Enumerating objects: 2829, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 2829 (delta 6), reused 10 (delta 2), pack-reused 2807 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2829/2829), 106.98 MiB | 27.63 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WangZixuan-nus/deep-learning.git\n",
        "\n",
        "\n",
        "# Define the data directories\n",
        "TRAIN_DIR = \"/content/deep-learning/projects/fruit-recognition/fruitdata/train\"\n",
        "TEST_DIR = \"/content/deep-learning/projects/fruit-recognition/fruitdata/test\"\n",
        "AUGMENTED_TRAIN_DIR = \"/content/augmented_data/train\"\n",
        "AUGMENTED_TEST_DIR = \"/content/augmented_data/test\"\n",
        "MODEL_FORMAT = \"keras\"\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"\n",
        "    Creates necessary directories for saving results and models\n",
        "    \"\"\"\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(\"/content/augmented_data/train\", exist_ok=True)\n",
        "    os.makedirs(\"/content/augmented_data/test\", exist_ok=True)\n",
        "    os.makedirs(\"/content/models\", exist_ok=True)\n",
        "    os.makedirs(\"/content/results\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eLO-MKYRfFWf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def augment_data(source_dir, target_dir, augmentation_factor=3):\n",
        "    \"\"\"\n",
        "    Augments images in source_dir and saves them to target_dir\n",
        "\n",
        "    Parameters:\n",
        "    - source_dir: Directory containing original images\n",
        "    - target_dir: Directory where augmented images will be saved\n",
        "    - augmentation_factor: Number of augmented images to generate per original image\n",
        "    \"\"\"\n",
        "    print(f\"Augmenting data from {source_dir} to {target_dir}...\")\n",
        "\n",
        "    # Define the image data generator with various transformations\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,        # Random rotation in the range of 40 degrees\n",
        "        width_shift_range=0.2,    # Random horizontal shift up to 20%\n",
        "        height_shift_range=0.2,   # Random vertical shift up to 20%\n",
        "        shear_range=0.2,          # Random shearing transformations\n",
        "        zoom_range=0.2,           # Random zoom up to 20%\n",
        "        horizontal_flip=True,     # Randomly flip images horizontally\n",
        "        fill_mode='nearest'       # Fill in newly created pixels using nearest neighbor approach\n",
        "    )\n",
        "\n",
        "    # Process each class directory\n",
        "    for class_name in os.listdir(source_dir):\n",
        "        # Skip non-directory files\n",
        "        if not os.path.isdir(os.path.join(source_dir, class_name)):\n",
        "            continue\n",
        "\n",
        "        # Create class directory in target_dir if it doesn't exist\n",
        "        class_target_dir = os.path.join(target_dir, class_name)\n",
        "        os.makedirs(class_target_dir, exist_ok=True)\n",
        "\n",
        "        # Copy original images first\n",
        "        class_source_dir = os.path.join(source_dir, class_name)\n",
        "        for img_name in os.listdir(class_source_dir):\n",
        "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                img_path = os.path.join(class_source_dir, img_name)\n",
        "                # Load the image\n",
        "                img = load_img(img_path)\n",
        "                # Convert to array\n",
        "                x = img_to_array(img)\n",
        "                # Reshape to (1, height, width, channels)\n",
        "                x = x.reshape((1,) + x.shape)\n",
        "\n",
        "                # Copy the original image\n",
        "                tf.keras.utils.save_img(\n",
        "                    os.path.join(class_target_dir, img_name),\n",
        "                    x[0]\n",
        "                )\n",
        "\n",
        "                # Generate augmented images\n",
        "                i = 0\n",
        "                for batch in datagen.flow(\n",
        "                    x,\n",
        "                    batch_size=1,\n",
        "                    save_to_dir=class_target_dir,\n",
        "                    save_prefix=f\"{os.path.splitext(img_name)[0]}_aug\",\n",
        "                    save_format='jpg'\n",
        "                ):\n",
        "                    i += 1\n",
        "                    if i >= augmentation_factor:\n",
        "                        break\n",
        "\n",
        "    print(f\"Data augmentation completed. Augmented images saved to {target_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "icZZFapRdwhj"
      },
      "outputs": [],
      "source": [
        "def data_load(data_dir, test_data_dir, img_height, img_width, batch_size):\n",
        "    \"\"\"\n",
        "    Loads the training and testing datasets\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir: Directory containing training images\n",
        "    - test_data_dir: Directory containing testing images\n",
        "    - img_height: Target image height\n",
        "    - img_width: Target image width\n",
        "    - batch_size: Batch size for training/testing\n",
        "\n",
        "    Returns:\n",
        "    - train_ds: Training dataset\n",
        "    - val_ds: Validation/testing dataset\n",
        "    - class_names: List of class names\n",
        "    \"\"\"\n",
        "    # Load training dataset\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        label_mode='categorical',\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Load testing dataset\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        test_data_dir,\n",
        "        label_mode='categorical',\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Get class names from the training dataset\n",
        "    class_names = train_ds.class_names\n",
        "\n",
        "    # Use caching and prefetching for better performance\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PEnOGYq9fIOC"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(img_shape=(224, 224, 3), class_num=15):\n",
        "    \"\"\"\n",
        "    Builds and compiles a CNN model - fixed to use Input object properly\n",
        "\n",
        "    Parameters:\n",
        "    - img_shape: Shape of input images (height, width, channels)\n",
        "    - class_num: Number of classes to predict\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled CNN model\n",
        "    \"\"\"\n",
        "    # Create input layer explicitly\n",
        "    inputs = Input(shape=img_shape)\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    x = tf.keras.layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "    # First convolutional block\n",
        "    x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(2, 2)(x)\n",
        "\n",
        "    # Second convolutional block\n",
        "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(2, 2)(x)\n",
        "\n",
        "    # Third convolutional block\n",
        "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(2, 2)(x)\n",
        "\n",
        "    # Flatten the output for the dense layers\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    # Fully connected layer with 128 units\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    outputs = tf.keras.layers.Dense(class_num, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Display the model architecture\n",
        "    model.summary()\n",
        "\n",
        "    # Compile the model with SGD optimizer and categorical crossentropy loss\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qw_lmAxHfL76"
      },
      "outputs": [],
      "source": [
        "def build_mobilenet_model(img_shape=(224, 224, 3), class_num=15):\n",
        "    \"\"\"\n",
        "    Builds and compiles a MobileNetV2-based model using transfer learning\n",
        "\n",
        "    Parameters:\n",
        "    - img_shape: Shape of input images (height, width, channels)\n",
        "    - class_num: Number of classes to predict\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled MobileNetV2 model\n",
        "    \"\"\"\n",
        "    # Create input layer explicitly\n",
        "    inputs = Input(shape=img_shape)\n",
        "\n",
        "    # Preprocessing layer to scale pixel values to [-1, 1] as expected by MobileNetV2\n",
        "    x = tf.keras.layers.Rescaling(1./127.5, offset=-1)(inputs)\n",
        "\n",
        "    # Load pre-trained MobileNetV2 without the top classifier\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=img_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Freeze the base model so its weights won't be updated during training\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Apply the base model\n",
        "    x = base_model(x)\n",
        "\n",
        "    # Global average pooling to reduce spatial dimensions\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    outputs = tf.keras.layers.Dense(class_num, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Display the model architecture\n",
        "    model.summary()\n",
        "\n",
        "    # Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w3fEAFtOfNxd"
      },
      "outputs": [],
      "source": [
        "def show_accuracy_and_loss(history, model_name=\"model\"):\n",
        "    \"\"\"\n",
        "    Plots the training and validation accuracy and loss\n",
        "\n",
        "    Parameters:\n",
        "    - history: History object returned by model.fit()\n",
        "    - model_name: Name of the model for saving the plot\n",
        "    \"\"\"\n",
        "    # Extract metrics from history\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()), 1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.tight_layout()\n",
        "    filename = f\"/content/results/results_{model_name}.png\"\n",
        "    plt.savefig(filename, dpi=100)\n",
        "    print(f\"Training results plot saved to {filename}\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "znurn_63fPuR"
      },
      "outputs": [],
      "source": [
        "def train_model(model_type=\"cnn\", epochs=15):\n",
        "    \"\"\"\n",
        "    Trains either a CNN or MobileNet model\n",
        "\n",
        "    Parameters:\n",
        "    - model_type: Type of model to train (\"cnn\" or \"mobilenet\")\n",
        "    - epochs: Number of training epochs\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load data from augmented directories\n",
        "    train_ds, val_ds, class_names = data_load(\n",
        "        AUGMENTED_TRAIN_DIR,\n",
        "        AUGMENTED_TEST_DIR,\n",
        "        224, 224, 16\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model_type.upper()} model with {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "    # Build the model based on type\n",
        "    if model_type == \"cnn\":\n",
        "        model = build_cnn_model(class_num=len(class_names))\n",
        "        model_path = f\"/content/models/cnn_model.{MODEL_FORMAT}\"\n",
        "    else:\n",
        "        model = build_mobilenet_model(class_num=len(class_names))\n",
        "        model_path = f\"/content/models/mobilenet_model.{MODEL_FORMAT}\"\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    # Save the model using modern format\n",
        "    model.save(model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Calculate training time\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    print(f'Training completed in {duration:.2f} seconds')\n",
        "\n",
        "    # Show and save training progress\n",
        "    show_accuracy_and_loss(history, model_type)\n",
        "\n",
        "    return model, class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rBtoxWL1fRd7"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model_path, model_name):\n",
        "    \"\"\"\n",
        "    Evaluates a trained model and generates a confusion matrix heatmap\n",
        "\n",
        "    Parameters:\n",
        "    - model_path: Path to the saved model\n",
        "    - model_name: Name of the model for saving results\n",
        "    \"\"\"\n",
        "    # Load dataset and model\n",
        "    _, test_ds, class_names = data_load(\n",
        "        AUGMENTED_TRAIN_DIR,\n",
        "        AUGMENTED_TEST_DIR,\n",
        "        224, 224, 16\n",
        "    )\n",
        "\n",
        "    # Load the model - with optimization to avoid built metrics warning\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Evaluate model on test data - this will build the metrics\n",
        "    loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "    print(f'{model_name.upper()} Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Collect predictions and true labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    # Disable verbose output for predictions\n",
        "    for images, labels in test_ds:\n",
        "        true_batch = labels.numpy()\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "\n",
        "        true_indices = np.argmax(true_batch, axis=1)\n",
        "        predicted_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "        true_labels.extend(true_indices)\n",
        "        predicted_labels.extend(predicted_indices)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    num_classes = len(class_names)\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        confusion_matrix[true][pred] += 1\n",
        "\n",
        "    # Normalize by row (true labels)\n",
        "    row_sums = confusion_matrix.sum(axis=1, keepdims=True)\n",
        "    normalized_matrix = confusion_matrix / row_sums\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    ax = plt.subplot()\n",
        "    im = ax.imshow(normalized_matrix, cmap=\"OrRd\")\n",
        "\n",
        "    # Add labels\n",
        "    ax.set_xticks(np.arange(len(class_names)))\n",
        "    ax.set_yticks(np.arange(len(class_names)))\n",
        "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    ax.set_yticklabels(class_names)\n",
        "\n",
        "    # Add text annotations\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            text = ax.text(j, i, f\"{normalized_matrix[i, j]:.2f}\",\n",
        "                           ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "    ax.set_xlabel(\"Predicted Label\")\n",
        "    ax.set_ylabel(\"True Label\")\n",
        "    ax.set_title(f\"Confusion Matrix - {model_name.upper()}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.colorbar(im)\n",
        "\n",
        "    # Save heatmap\n",
        "    filename = f\"/content/results/heatmap_{model_name}.png\"\n",
        "    plt.savefig(filename, dpi=100)\n",
        "    print(f\"Confusion matrix heatmap saved to {filename}\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r2lmKjMIfTlE"
      },
      "outputs": [],
      "source": [
        "def compare_models():\n",
        "    \"\"\"\n",
        "    Compares the performance of CNN and MobileNet models\n",
        "    \"\"\"\n",
        "    # Load results - paths updated to use the modern format\n",
        "    cnn_model_path = f\"/content/models/cnn_model.{MODEL_FORMAT}\"\n",
        "    mobilenet_model_path = f\"/content/models/mobilenet_model.{MODEL_FORMAT}\"\n",
        "\n",
        "    # Load dataset\n",
        "    _, test_ds, _ = data_load(\n",
        "        AUGMENTED_TRAIN_DIR,\n",
        "        AUGMENTED_TEST_DIR,\n",
        "        224, 224, 16\n",
        "    )\n",
        "\n",
        "    # Load models\n",
        "    cnn_model = tf.keras.models.load_model(cnn_model_path)\n",
        "    mobilenet_model = tf.keras.models.load_model(mobilenet_model_path)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\nEvaluating CNN model...\")\n",
        "    cnn_loss, cnn_accuracy = cnn_model.evaluate(test_ds, verbose=1)\n",
        "\n",
        "    print(\"\\nEvaluating MobileNetV2 model...\")\n",
        "    mobilenet_loss, mobilenet_accuracy = mobilenet_model.evaluate(test_ds, verbose=1)\n",
        "\n",
        "    # Create comparison plot\n",
        "    models = ['CNN', 'MobileNetV2']\n",
        "    accuracies = [cnn_accuracy, mobilenet_accuracy]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(models, accuracies, color=['#3498db', '#2ecc71'])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Accuracy Comparison')\n",
        "\n",
        "    # Add text labels on bars\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
        "\n",
        "    # Save comparison\n",
        "    filename = \"/content/results/model_comparison.png\"\n",
        "    plt.savefig(filename, dpi=100)\n",
        "    print(f\"Model comparison saved to {filename}\")\n",
        "    plt.close()\n",
        "\n",
        "    # Print comparison results\n",
        "    print(\"\\nModel Comparison Results:\")\n",
        "    print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
        "    print(f\"MobileNetV2 Accuracy: {mobilenet_accuracy:.4f}\")\n",
        "    print(f\"Difference: {abs(cnn_accuracy - mobilenet_accuracy):.4f}\")\n",
        "\n",
        "    if cnn_accuracy > mobilenet_accuracy:\n",
        "        print(\"CNN model performs better on this dataset.\")\n",
        "    elif mobilenet_accuracy > cnn_accuracy:\n",
        "        print(\"MobileNetV2 model performs better on this dataset.\")\n",
        "    else:\n",
        "        print(\"Both models perform equally on this dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TAXxIMZ5fVgH",
        "outputId": "9f833fd2-69e2-4480-afde-68fe9c330f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting data from /content/deep-learning/projects/fruit-recognition/fruitdata/train to /content/augmented_data/train...\n",
            "Data augmentation completed. Augmented images saved to /content/augmented_data/train\n",
            "Augmenting data from /content/deep-learning/projects/fruit-recognition/fruitdata/test to /content/augmented_data/test...\n",
            "Data augmentation completed. Augmented images saved to /content/augmented_data/test\n",
            "\n",
            "===== Training CNN Model =====\n",
            "Found 8679 files belonging to 15 classes.\n",
            "Found 1602 files belonging to 15 classes.\n",
            "Training CNN model with 15 classes: ['Hami melon', 'bitter gourd', 'carrot', 'cherry', 'cucumber', 'dragon fruit', 'durian', 'kiwi fruit', 'lemon', 'litchi', 'longan', 'mango', 'pear', 'pineapple', 'strawberry']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43264\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,537,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │         \u001b[38;5;34m1,935\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43264</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,537,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,563,439\u001b[0m (21.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,563,439</span> (21.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,563,439\u001b[0m (21.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,563,439</span> (21.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 789ms/step - accuracy: 0.1248 - loss: 2.5743 - val_accuracy: 0.2459 - val_loss: 2.4285\n",
            "Epoch 2/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 736ms/step - accuracy: 0.3476 - loss: 1.8920 - val_accuracy: 0.3564 - val_loss: 2.1111\n",
            "Epoch 3/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 745ms/step - accuracy: 0.4651 - loss: 1.5668 - val_accuracy: 0.4151 - val_loss: 1.9610\n",
            "Epoch 4/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 745ms/step - accuracy: 0.5424 - loss: 1.3623 - val_accuracy: 0.4276 - val_loss: 1.9350\n",
            "Epoch 5/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 737ms/step - accuracy: 0.6091 - loss: 1.1702 - val_accuracy: 0.5019 - val_loss: 1.6786\n",
            "Epoch 6/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 735ms/step - accuracy: 0.6743 - loss: 0.9858 - val_accuracy: 0.5287 - val_loss: 1.6411\n",
            "Epoch 7/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 739ms/step - accuracy: 0.7335 - loss: 0.7982 - val_accuracy: 0.5412 - val_loss: 1.7077\n",
            "Epoch 8/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 739ms/step - accuracy: 0.7991 - loss: 0.6138 - val_accuracy: 0.5468 - val_loss: 1.8373\n",
            "Epoch 9/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 736ms/step - accuracy: 0.8683 - loss: 0.4239 - val_accuracy: 0.5094 - val_loss: 2.1328\n",
            "Epoch 10/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 745ms/step - accuracy: 0.9052 - loss: 0.3097 - val_accuracy: 0.5549 - val_loss: 1.9477\n",
            "Epoch 11/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 741ms/step - accuracy: 0.9325 - loss: 0.2425 - val_accuracy: 0.4588 - val_loss: 3.5001\n",
            "Epoch 12/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 733ms/step - accuracy: 0.9594 - loss: 0.1542 - val_accuracy: 0.5581 - val_loss: 2.2822\n",
            "Epoch 13/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 736ms/step - accuracy: 0.9786 - loss: 0.0910 - val_accuracy: 0.5874 - val_loss: 2.1407\n",
            "Epoch 14/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 733ms/step - accuracy: 0.9950 - loss: 0.0315 - val_accuracy: 0.5787 - val_loss: 2.3601\n",
            "Epoch 15/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 719ms/step - accuracy: 0.9952 - loss: 0.0234 - val_accuracy: 0.5893 - val_loss: 2.4832\n",
            "Model saved to /content/models/cnn_model.keras\n",
            "Training completed in 6206.79 seconds\n",
            "Training results plot saved to /content/results/results_cnn.png\n",
            "\n",
            "===== Training MobileNet Model =====\n",
            "Found 8679 files belonging to 15 classes.\n",
            "Found 1602 files belonging to 15 classes.\n",
            "Training MOBILENET model with 15 classes: ['Hami melon', 'bitter gourd', 'carrot', 'cherry', 'cucumber', 'dragon fruit', 'durian', 'kiwi fruit', 'lemon', 'litchi', 'longan', 'mango', 'pear', 'pineapple', 'strawberry']\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │        \u001b[38;5;34m19,215\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,277,199\u001b[0m (8.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,199</span> (8.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,215\u001b[0m (75.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> (75.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 728ms/step - accuracy: 0.7033 - loss: 1.0198 - val_accuracy: 0.8826 - val_loss: 0.3833\n",
            "Epoch 2/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 712ms/step - accuracy: 0.9569 - loss: 0.1644 - val_accuracy: 0.8945 - val_loss: 0.3437\n",
            "Epoch 3/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 716ms/step - accuracy: 0.9855 - loss: 0.0858 - val_accuracy: 0.8995 - val_loss: 0.3314\n",
            "Epoch 4/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 765ms/step - accuracy: 0.9937 - loss: 0.0526 - val_accuracy: 0.9026 - val_loss: 0.3263\n",
            "Epoch 5/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 757ms/step - accuracy: 0.9974 - loss: 0.0349 - val_accuracy: 0.9026 - val_loss: 0.3241\n",
            "Epoch 6/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 724ms/step - accuracy: 0.9996 - loss: 0.0242 - val_accuracy: 0.9064 - val_loss: 0.3237\n",
            "Epoch 7/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 722ms/step - accuracy: 0.9999 - loss: 0.0174 - val_accuracy: 0.9082 - val_loss: 0.3250\n",
            "Epoch 8/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 726ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.9089 - val_loss: 0.3277\n",
            "Epoch 9/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 719ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9107 - val_loss: 0.3315\n",
            "Epoch 10/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 721ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9114 - val_loss: 0.3363\n",
            "Epoch 11/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 723ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9107 - val_loss: 0.3418\n",
            "Epoch 12/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 723ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.9107 - val_loss: 0.3479\n",
            "Epoch 13/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 721ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9114 - val_loss: 0.3545\n",
            "Epoch 14/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 721ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9095 - val_loss: 0.3614\n",
            "Epoch 15/15\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 770ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9095 - val_loss: 0.3687\n",
            "Model saved to /content/models/mobilenet_model.keras\n",
            "Training completed in 6130.96 seconds\n",
            "Training results plot saved to /content/results/results_mobilenet.png\n",
            "\n",
            "===== Evaluating CNN Model =====\n",
            "Found 8679 files belonging to 15 classes.\n",
            "Found 1602 files belonging to 15 classes.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 281ms/step - accuracy: 0.5829 - loss: 2.4927\n",
            "CNN Test Accuracy: 0.5893\n",
            "Confusion matrix heatmap saved to /content/results/heatmap_cnn.png\n",
            "\n",
            "===== Evaluating MobileNet Model =====\n",
            "Found 8679 files belonging to 15 classes.\n",
            "Found 1602 files belonging to 15 classes.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 620ms/step - accuracy: 0.9118 - loss: 0.3292\n",
            "MOBILENET Test Accuracy: 0.9095\n",
            "Confusion matrix heatmap saved to /content/results/heatmap_mobilenet.png\n",
            "\n",
            "===== Comparing Models =====\n",
            "Found 8679 files belonging to 15 classes.\n",
            "Found 1602 files belonging to 15 classes.\n",
            "\n",
            "Evaluating CNN model...\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 237ms/step - accuracy: 0.5829 - loss: 2.4927\n",
            "\n",
            "Evaluating MobileNetV2 model...\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 588ms/step - accuracy: 0.9118 - loss: 0.3292\n",
            "Model comparison saved to /content/results/model_comparison.png\n",
            "\n",
            "Model Comparison Results:\n",
            "CNN Accuracy: 0.5893\n",
            "MobileNetV2 Accuracy: 0.9095\n",
            "Difference: 0.3202\n",
            "MobileNetV2 model performs better on this dataset.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the complete workflow\n",
        "    \"\"\"\n",
        "    # Setup directories\n",
        "    setup_directories()\n",
        "\n",
        "    # Augment training and testing data\n",
        "    augment_data(TRAIN_DIR, AUGMENTED_TRAIN_DIR, augmentation_factor=3)\n",
        "    augment_data(TEST_DIR, AUGMENTED_TEST_DIR, augmentation_factor=2)\n",
        "\n",
        "    # Train CNN model\n",
        "    print(\"\\n===== Training CNN Model =====\")\n",
        "    _, _ = train_model(model_type=\"cnn\", epochs=15)\n",
        "\n",
        "    # Train MobileNet model\n",
        "    print(\"\\n===== Training MobileNet Model =====\")\n",
        "    _, _ = train_model(model_type=\"mobilenet\", epochs=15)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\n===== Evaluating CNN Model =====\")\n",
        "    evaluate_model(f\"/content/models/cnn_model.{MODEL_FORMAT}\", \"cnn\")\n",
        "\n",
        "    print(\"\\n===== Evaluating MobileNet Model =====\")\n",
        "    evaluate_model(f\"/content/models/mobilenet_model.{MODEL_FORMAT}\", \"mobilenet\")\n",
        "\n",
        "    # Compare models\n",
        "    print(\"\\n===== Comparing Models =====\")\n",
        "    compare_models()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}